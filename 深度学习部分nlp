本次包含了word2vec；textcnn，textrnn。代码来自datawhale组织。后面会考虑复写为tf。
##
Word2vec 是 Word Embedding 方式之一，属于 NLP 领域。他是将词转化为“可计算”“结构化”的向量的过程。包含CBOW以及skip-gram模型
##
TextCnn的结构
1. 嵌入层(embedding layer)
textcnn使用预先训练好的词向量作embedding layer。对于数据集里的所有词，因为每个词都可以表征成一个向量，因此我们可以得到一个嵌入矩阵M, M里的每一行都是词向量。这个M可以是静态(static)的，也就是固定不变。可以是非静态(non-static)的，也就是可以根据反向传播更新
2. 卷积池化层(convolution and pooling)
卷积(convolution)
输入一个句子，首先对这个句子进行切词，假设有s个单词。对每个词，跟句嵌入矩阵M, 可以得到词向量。假设词向量一共有d维。那么对于这个句子，便可以得到s行d列的矩阵AϵRs×d.
我们可以把矩阵A看成是一幅图像，使用卷积神经网络去提取特征。由于句子中相邻的单词关联性总是很高的，因此可以使用一维卷积。卷积核的宽度就是词向量的维度d，高度是超参数，可以设置。
现在假设有一个卷积核，是一个宽度为d，高度为h的矩阵w，那么w有h∗d个参数需要被更新。对于一个句子，经过嵌入层之后可以得到矩阵AϵRs×d. A[i:j]表示A的第i行到第j行, 那么卷积操作可以用如下公式表示：
3.池化(pooling)
不同尺寸的卷积核得到的特征(feature map)大小也是不一样的，因此我们对每个feature map使用池化函数，使它们的维度相同。最常用的就是1-max pooling，提取出feature map照片那个的最大值。这样每一个卷积核得到特征就是一个值，对所有卷积核使用1-max pooling，再级联起来，可以得到最终的特征向量，这个特征向量再输入softmax layer做分类。这个地方可以使用drop out防止过拟合
 参数选择
根据文章中的描述，总结如下。
1. 初始化词向量
使用word2vec和golve都可以，不要使用one-hot vectors
2. 卷积核的尺寸
1-10之间，具体情况具体分析，对最终结果影响较大。一般来讲，句子长度越长，卷积核的尺寸越大。
3. 每种尺寸卷积核的数量
100-600之间，对模型性能影响较大，需要注意的是增加卷积核的数量会增加训练模型的实践。
4. 激活函数的选择
使用relu函数
5. drop out rate
0.0-0.5, 当增加卷积核的数量时，可以尝试增加drop out rate，甚至可以大于0.5
6. 池化的选择
1-max pooling
7. 正则项
正则项对最终模型性能的影响很小
##textrnn
有单向rnn，双向rnn和lstm模型
#######
word2vec
#######
%%time
clf = LogisticRegression(C=4, n_jobs=16)
clf.fit(x_train_, y_train_)

y_pred = clf.predict(x_valid_)
train_scores = clf.score(x_train_, y_train_)
print(train_scores, f1_score(y_pred, y_valid_, average='macro'))
import logging
import random

import numpy as np
import torch

logging.basicConfig(level=logging.INFO, format='%(asctime)-15s %(levelname)s: %(message)s')

# set seed 
seed = 666
random.seed(seed)
np.random.seed(seed)
torch.cuda.manual_seed(seed)
torch.manual_seed(seed)
fold_num = 10
data_file = 'datalab/72510/train_set.csv'
import pandas as pd
